{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBw\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9dab1e4-7623-4a13-9c80-503370f6ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja funkcji `chat`, która obsługuje rozmowę z modelem OpenAI\n",
    "def chat(message, history):\n",
    "    # Tworzymy listę `messages`, która zawiera:\n",
    "    # 1. Wiadomość systemową (zdefiniowaną wcześniej jako `system_message`).\n",
    "    # 2. Historię rozmowy (przekazaną do funkcji jako `history`).\n",
    "    # 3. Nową wiadomość użytkownika (`message`).\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    # Wypisujemy historię rozmowy do konsoli (pomocne przy debugowaniu)\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "\n",
    "    # Wypisujemy pełną listę `messages` po dodaniu nowej wiadomości\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    # Wysyłamy przygotowaną listę `messages` do OpenAI,\n",
    "    # używając modelu `MODEL` oraz włączając tryb strumieniowania `stream=True`.\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    # Tworzymy pusty string, który będzie stopniowo uzupełniany odpowiedzią modelu.\n",
    "    response = \"\"\n",
    "    \n",
    "    # Iterujemy po kolejnych fragmentach odpowiedzi zwracanych przez OpenAI.\n",
    "    for chunk in stream:\n",
    "        # Każdy fragment (`chunk`) zawiera obiekt `choices`, z którego pobieramy tekst odpowiedzi.\n",
    "        # Jeśli `delta.content` nie zawiera tekstu (może być `None`), dodajemy pusty string `''`.\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        \n",
    "        # Używamy `yield`, aby zwracać częściową odpowiedź na bieżąco.\n",
    "        # Dzięki temu możemy wyświetlać wynik w czasie rzeczywistym.\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'cześć', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Cześć! Jak mogę Ci dzisiaj pomóc w naszym sklepie? Szukasz czegoś szczególnego?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'szukam tanich telefonów', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Rozumiem, że szukasz telefonów, ale w naszym sklepie specjalizujemy się w odzieży i akcesoriach. Może zainteresuje Cię coś na naszych wyprzedażach? Mamy świetne oferty na kapelusze w wysokości 60% zniżki oraz na inne ubrania, które są przecenione o 50%! Jeśli chcesz, mogę pokazać Ci nasze najnowsze modele kapeluszy. Co o tym myślisz?', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\"}, {'role': 'user', 'metadata': None, 'content': 'cześć', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Cześć! Jak mogę Ci dzisiaj pomóc w naszym sklepie? Szukasz czegoś szczególnego?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'szukam tanich telefonów', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Rozumiem, że szukasz telefonów, ale w naszym sklepie specjalizujemy się w odzieży i akcesoriach. Może zainteresuje Cię coś na naszych wyprzedażach? Mamy świetne oferty na kapelusze w wysokości 60% zniżki oraz na inne ubrania, które są przecenione o 50%! Jeśli chcesz, mogę pokazać Ci nasze najnowsze modele kapeluszy. Co o tym myślisz?', 'options': None}, {'role': 'user', 'content': 'podaj nazwę kapel;usza ktory jest najtańszy'}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\"}, {'role': 'user', 'content': 'cześć'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'cześć', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Cześć! Jak mogę Ci dzisiaj pomóc w naszym sklepie? Szukasz czegoś szczególnego?', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\"}, {'role': 'user', 'metadata': None, 'content': 'cześć', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Cześć! Jak mogę Ci dzisiaj pomóc w naszym sklepie? Szukasz czegoś szczególnego?', 'options': None}, {'role': 'user', 'content': 'szukam tanich telefonów'}]\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c0ac9-e0f2-4c4e-a1b6-a35da3e7d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja `chat` obsługuje komunikację z modelem językowym OpenAI, umożliwiając interaktywną rozmowę.\n",
    "def chat(message, history):\n",
    "    # Tworzymy listę `messages`, która zawiera:\n",
    "    # 1. Wiadomość systemową (określa kontekst rozmowy i zachowanie modelu).\n",
    "    # 2. Historię wcześniejszych wiadomości użytkownika i asystenta.\n",
    "    # 3. Nową wiadomość użytkownika.\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    # Wysyłamy zapytanie do OpenAI, przekazując:\n",
    "    # - `model=MODEL` – określa, który model OpenAI zostanie użyty.\n",
    "    # - `messages=messages` – przekazujemy pełen kontekst rozmowy.\n",
    "    # - `stream=True` – aktywujemy strumieniowanie, co pozwala na stopniowe wyświetlanie odpowiedzi.\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    # Inicjalizujemy pusty string `response`, który będzie przechowywał wygenerowaną odpowiedź.\n",
    "    response = \"\"\n",
    "\n",
    "    # Iterujemy po kolejnych fragmentach odpowiedzi zwracanych przez OpenAI.\n",
    "    for chunk in stream:\n",
    "        # Pobieramy część tekstu wygenerowaną przez model i dodajemy ją do `response`.\n",
    "        # `delta.content` może być `None`, więc stosujemy `or ''`, aby uniknąć błędów.\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "\n",
    "        # Używamy `yield`, co pozwala zwracać częściową odpowiedź na bieżąco.\n",
    "        # Dzięki temu użytkownik widzi tekst stopniowo, zamiast czekać na pełną odpowiedź.\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats!\"}, {'role': 'user', 'content': 'co masz w ofercie'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'co masz w ofercie', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Mamy wiele wspaniałych rzeczy w ofercie! Oferujemy odzież, dodatki i wiele więcej. Szczególnie chciałbym zwrócić uwagę na nasze kapelusze, które są obecnie w promocji - aż 60% taniej! Jeśli szukasz czegoś wyjątkowego, warto je wypróbować. Oprócz tego mamy wiele innych przedmiotów z rabatem 50%. Czy jest coś konkretnego, czego szukasz?', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats!\"}, {'role': 'user', 'metadata': None, 'content': 'co masz w ofercie', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Mamy wiele wspaniałych rzeczy w ofercie! Oferujemy odzież, dodatki i wiele więcej. Szczególnie chciałbym zwrócić uwagę na nasze kapelusze, które są obecnie w promocji - aż 60% taniej! Jeśli szukasz czegoś wyjątkowego, warto je wypróbować. Oprócz tego mamy wiele innych przedmiotów z rabatem 50%. Czy jest coś konkretnego, czego szukasz?', 'options': None}, {'role': 'user', 'content': 'czy masz buty'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'co masz w ofercie', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Mamy wiele wspaniałych rzeczy w ofercie! Oferujemy odzież, dodatki i wiele więcej. Szczególnie chciałbym zwrócić uwagę na nasze kapelusze, które są obecnie w promocji - aż 60% taniej! Jeśli szukasz czegoś wyjątkowego, warto je wypróbować. Oprócz tego mamy wiele innych przedmiotów z rabatem 50%. Czy jest coś konkretnego, czego szukasz?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'czy masz buty', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Mamy różne style w ofercie, ale niestety buty nie są dzisiaj w promocji. Jednak chciałbym przypomnieć, że mamy świetne kapelusze w sprzedaży, a ich cena jest aż o 60% niższa! Może zechciałbyś się przyjrzeć naszym kapeluszom? To świetny dodatek do każdej stylizacji!', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats!\"}, {'role': 'user', 'metadata': None, 'content': 'co masz w ofercie', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Mamy wiele wspaniałych rzeczy w ofercie! Oferujemy odzież, dodatki i wiele więcej. Szczególnie chciałbym zwrócić uwagę na nasze kapelusze, które są obecnie w promocji - aż 60% taniej! Jeśli szukasz czegoś wyjątkowego, warto je wypróbować. Oprócz tego mamy wiele innych przedmiotów z rabatem 50%. Czy jest coś konkretnego, czego szukasz?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'czy masz buty', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Mamy różne style w ofercie, ale niestety buty nie są dzisiaj w promocji. Jednak chciałbym przypomnieć, że mamy świetne kapelusze w sprzedaży, a ich cena jest aż o 60% niższa! Może zechciałbyś się przyjrzeć naszym kapeluszom? To świetny dodatek do każdej stylizacji!', 'options': None}, {'role': 'user', 'content': 'podaj nazwe butow ktore nie sa w promocji '}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb89544-8cd9-4be3-bcd6-d13f229bbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "\n",
    "    # 🔹 Inicjalizujemy zmienną `relevant_system_message`, przypisując jej domyślną wiadomość systemową.\n",
    "    relevant_system_message = system_message\n",
    "\n",
    "    # 🔹 Sprawdzamy, czy w wiadomości użytkownika pojawia się słowo \"belt\" (pasek).\n",
    "    #    Jeśli tak, dodajemy informację, że sklep nie sprzedaje pasków i sugerujemy inne produkty.\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    # 🔹 Tworzymy pełną listę wiadomości do wysłania do modelu:\n",
    "    #    1. Wiadomość systemowa (`relevant_system_message`) — określa zasady rozmowy.\n",
    "    #    2. Historia wcześniejszych wiadomości (`history`) — kontekst rozmowy.\n",
    "    #    3. Nowa wiadomość użytkownika (`message`).\n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    # 🔹 Wysyłamy dane do OpenAI:\n",
    "    #    - `model=MODEL` → wskazuje, który model AI wykorzystujemy.\n",
    "    #    - `messages=messages` → przekazujemy pełny kontekst rozmowy.\n",
    "    #    - `stream=True` → aktywuje tryb strumieniowania (odpowiedź zwracana stopniowo).\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    # 🔹 Inicjalizujemy pusty string `response`, który będzie przechowywał odpowiedź modelu.\n",
    "    response = \"\"\n",
    "\n",
    "    # 🔹 Iterujemy po kolejnych fragmentach odpowiedzi zwracanych przez model.\n",
    "    for chunk in stream:\n",
    "        # Dodajemy nową część odpowiedzi do zmiennej `response`.\n",
    "        # `delta.content` może być `None`, więc stosujemy `or ''`, aby uniknąć błędów.\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "\n",
    "        # Używamy `yield`, aby zwracać częściową odpowiedź na bieżąco.\n",
    "        # Dzięki temu użytkownik widzi tekst stopniowo, co poprawia płynność interakcji.\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a57ee0-b945-48a7-a024-01b56a5d4b3e",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business Applications</h2>\n",
    "            <span style=\"color:#181;\">Conversational Assistants are of course a hugely common use case for Gen AI, and the latest frontier models are remarkably good at nuanced conversation. And Gradio makes it easy to have a user interface. Another crucial skill we covered is how to use prompting to provide context, information and examples.\n",
    "<br/><br/>\n",
    "Consider how you could apply an AI Assistant to your business, and make yourself a prototype. Use the system prompt to give context on your business, and set the tone for the LLM.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
